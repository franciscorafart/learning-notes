# S3

## Use cases
- backup and storage
- Disaster recovery
- Archive
- Application hosting
- Media hosting
- Software delivery
- Static website

## S3 Buckets
- Amazon stores Objects (files) on Buckets (Directories)
- Buckets must have globally unique name (across all regions and all accounts)
- Buckets are created on a region, but S3 looks like a global service (buckets available in all regions)
- Naming convention: No uppercase, no underscore, 3-63 chars long, not an ip, start with lowercase or number

## Objects (files)
- Objects have a key
- The key is the FULL path to the file (Prefix + object name)
    For example: `s3://my-bucket/my_file.txt`
- No concept of directories, allthough UI makes it look like it
- Just jeys with long names that contain slashes
- Values are content of the body
- Max object size: 5TB
- If uploading big file (>5GB) use `multi-part upload`
- Metadata: list of text key / value pairs
- Tags (Unicode key / value pair) - useful for security / lifecycle
- Version ID
- Objects have a public URL and a signed Url.
    - When you hit the Open button, you see the file because it takes you to a signed URL that has your credentials encoded
    - When accessing the public url, the access can be denied

## Security

1. User-based (IAM policy)
IAM policies => Which API calls are authorized for a specific IAM user

2. Resource-Based security (Bucket Policy)
- Bucket-wide rules from the S3 console. This is where we make an S3 bucket public (most common)
- Object Access Control list (ACL) => Finer grained access policies (can be disabled)
- Bucket Access Control List (ACL)=> less common (can be disabled)

IAM can access bucket if:
- IAM permission allows it
OR
- Resource policy allows it
AND
- There isn't any explicit DENY access

3. Encryption
S3 content can be encrypted for further security

## S3 Bucket Policies
- JSON based policies
    - "Resource" specifies the resources affected by the policy
    - "Effect" specifies what actions we Allow or Deny
    - "Action" specifies what action is allowed (or denied)

We can use an S3 policy to grant public access to a bucket, grant access to another account, or encrypt file on upload. 

Examples:

S3 access permissions to an IAM user with an IAM policy.
S3 access to a EC2 instance, use an Ec2 instance role with the corresponding permissions.
Advanced: For cross account access, use Bucket Policy.

### Bucket settings for block public access
These were created to avoid company data leaks.

If your bucket should never be public, allways leave this ON
Can be set at the account level too if no S3 buckets should be public.


```
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Sid": "PublicReadGetObject",
			"Effect": "Allow",
			"Principal": "*",
			"Action": "s3:GetObject",
			"Resource": "arn:aws:s3:::francisco-test/*"
		}
	]
}
```

`"Resource": ["<arn_resource>/*"]` => `/*` says every element in the specified bucket is accessible. Becauase GetObject action applies to individual objects in the bucket.
`"Principal": "*"` => Anyone can access

### Static websites
S3 can host static websites.

For static sites, disable `Block public access` and add a Read Policy for the bucket.

#### Hands on static websites
1. Enable Static Website on properties
2. Specify entry point (index.html)

## S3 versioning
- You can version files on S3
It needs to be enabled at the bucket level

- What it does?
If we re-upload that file, it will create a version of it instead of deleting the old one.

- Why?
It is best practice to version buckets:
    - Protects against unintentional deleted (Marks the files to be deleted before deleting them)
    - Easy to roll back
    - Any file existing prior to versioning will have version = null
    - Suspending versioning doesn't delete previous versioning.

With the `Show Versions` toggle you can see the versions of the different files

## S3 Replication
CRR Cross Region replication
SRR Same region replication

The idea is to do asynchronous replication of one bucket to another. To do this:
- Enable versioning in source and destination buckets
- Enable SRR or CRR
- Buckets can be in different AWS accounts
- Must give proper IAM permissions to S3

Use Cases:
- CRR: Compliance, low-latency access, replication across accounts.
- SRR: Aggregate logs, live replication between production and test accounts

### Hands on
1. On original bucket go to `Management => Replication Rules` .
2. Create replication rule.
3. Choose a destination bucket (in same or different region)
4. Assign or create new IAM role

When you enable replication, by default it only replicates from the moment you set it up, not old Objects. When creating the replication rule you can do a batch operation to replicate all the existing objects.

## S3 storage classes (Must know them for exam)
- Standard- General purpose
    - 99.99% Availability
    - Used for frequently accessed objects
    - Low latency and high throughput
    - Use cases: Content, big-data analytics, mobile and gaming applications.
- Standard infrequent access
    - Less frequency but rapid access when needed
    - Lower cost than S3 standard but will have cost for retrieval
    - 99.9% availability
    - Use Case: Disaster recovery and backups
- One Zone-Infrequent access
    - High durability within single AZ. Data would be lost if AZ destroyed.
    - 99.5% Availability
    - Use Cases: Store secondary backup copies
- Glacier
    - Low cost storage for archiving and backup
    - Pay for storage and retrieval cost

    -  Glacier instant retrieval
        - Millisecond retrieval, great for data accessed once a quarter
        - Minimum storage duration (90 days)
    - Glacier flexible retreival
        - 3 flexibilities: Exedited (1-5 minutes), standard (3-5 hours), Bulk (5 - 12 hours) - free
        - Minimum storage duration 90 days
    - Glacier Deep Archive
        - For long term storage
        - Standard (12 hours), Bulk (48 hours)
        - Minimum storage duration 180 days
        - Lowest cost
- Intelligent Tiering
    - Small monthly monitoring and auto-tiering fee
    - Moves objects automatically between tiers based on usage.
    - No retrieval charges
    - Frequent Access Tier (default)
    - Infrequent access tier (Objects not accessed 30 days)
    - Archive instant access tier (Objects not accessed 90 days)
    - Archive access tier (Configurable from 90 to 700 days)
    - Deep Archive access tier (Configurable from 180 to 700 days)

We can move classes manually or using S3 Lifecylce configurations

### Durability and Availability
High durability of objects (99.999999999%)
Same durability for all storage classes

Availability
- Measures how readily available a service is
- Varies depending of storage class
- S3 standard 99.99% availability (Not available for 53 minutes a year)

### Hands on
To access storage class go to `Bucket => Properties => Storage classes`

## S3 encryption (1 question in exam)
- Server-side encryption => Objects uploaded will be encrypted by server when receiving the file. This is done by default.
- Client-side encryption => When user encrypts file before uploading it.

Both are possible on S3 but only Server-side encryption is by default.

## IAM access Analyzer for S3 (1 question in exam)
- Monitoring feature => Ensure only intended people have access to an S3 bucket
It evaluates S3 Bucket policies, S3 ACLs, and S3 Access points.

Powered by IAM access analyzer

## Shared responsability model for S3

AWS:
- All infrasctructure, Durability, Availability
- Configuration and vulnerability analysis
- Compliance validation

User:
- S3 versioning
- S3 bucket policies
- S3 replication
- Loggin and monitoring
- S3 storage class (cost)
- Data encryption

## AWS Snow Family


